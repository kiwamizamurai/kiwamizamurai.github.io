[{"content":"Abstruct In this post, I am going to explain how mathematics is involved with Neural Network and how to implement it with only numpy.\nContent this post was written when I was a junior student. Therefore, there might be some mistakes. Please leave the comment below.\nPrerequisites  rudimentary calculus knowledge  Chain Rule Jacobi-Matrix    This sections help you learn to take derivatives of vectors, matrices, and higher order tensors, thereby making you understand the following part.\nderivative As you know, this equation holds.\n$$\\begin{array}{c}\\vec{y}=\\vec{x} W \\\\ \\frac{d \\vec{y}}{d \\vec{x}}=W \\end{array}$$\nWhat is the derivative? What does it mean? In short, derivative is how sensitivie the specific variable with regard to a certain variable is. Jacobian matrix is a good example.\n$$J_f(x) = \\left[\\frac{\\partial f}{\\partial x_{1}} \\cdots \\frac{\\partial f}{\\partial x_{n}}\\right] = \\left[\\begin{array}{ccc}\\frac{\\partial f_{1}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_{m}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_{m}}{\\partial x_{n}} \\end{array}\\right]$$\nThe shape is $m \\times n$ due to the fact that the number of all combination of variables is $m \\times n$.\nvecotr by matrix What happens if I differentiate a vector by a matrix. The possible number of combinations of each variable will be\n$$\\texttt{length of vector} \\times \\texttt{shape of matrix}$$\n(vector varies along one coordinate while matrix varies along two coordinates)\nLet\u0026rsquo;s validate this step by step\n$$\\vec{y}_{3} = \\vec{x}_{1} W_{1,3} + \\vec{x}_{2} W_{2,3} + \\ldots + \\vec{x}_{D} W_{D, 3}$$\n$$\\frac{\\partial \\vec{y}_{3}}{\\partial W_{7,8}}=0 ~~~~ \\frac{\\partial \\vec{y}_{3}}{\\partial W_{2,3}}=\\vec{x}_{2}$$\nIn general, when the index of the $\\vec{y}$ component is equal to the second index of $W$, the derivative will be non-zero, but will be zero otherwise. We can write:\n$$\\frac{\\partial\\vec{y}_{j}}{\\partial W_{i, j}} = \\vec{x}_{i}$$\nbut the other elements of the 3-d array will be 0. If we let $F$ represent the 3d array representing the derivative of $\\vec{y}$ with respect to $W$, where\n$$F_{i, j, k}=\\frac{\\partial \\vec{y}_{i}}{\\partial W_{j,k}}$$\nthen\n$$F_{i, j, i}=\\vec{x}_{j}$$\nThe thing is that all other entries of $F$ are actually zero. Finally, if we define a new two-dimensional array G as\n$$G_{i, j}=F_{i, j, i}$$\nwe can see that all of the information we need about $F$ can be stored in $G$, and that the non-trivial portion of $F$ is really two-dimensional, not three-dimensional.\nmatrix by matrix In implementations of machine learning, we have to deal with a matrix instead of a single vector.\nLet’s assume that each individual $\\vec{x}$ is a row vector of length $D$, and that $X$ is a two-dimensional array with $N$ rows and $D$ columns. $W$, as in our last example, will be a matrix with $D$ rows and $C$ columns. $Y$ , given by\n$$ Y=X W $$\nwill also be a matrix, with $N$ rows and $C$ columns. Thus, each row of $Y$ will give a row vector associated with the corresponding row of the input $X$.\n$$ \\frac{\\partial Y_{a, b}}{\\partial X_{c, d}} = \\begin{cases} 0 \u0026amp; (a \\neq c) \\\\ not ~ 0 \u0026amp; (otherwise) \\end{cases} $$\nFurthermore, we can see that\n$$\\frac{\\partial Y_{i, j}}{\\partial X_{i, k}}=\\frac{\\partial }{\\partial X_{i, k}} \\left(\\sum_{k=1}^{D} X_{i, k} W_{k, j}\\right)=W_{k, j}$$\ndoesn’t depend at all upon which row of Y and X we are comparing. This is the same as\n$$ \\frac{\\partial Y_{i,:}}{\\partial X_{i,:}}=W^T $$\nseutp I assume the following architecture.\n$$ \\begin{array}{c} \\text { True: } t \\\\ \\text { Loss: } \\mathbb{E}[t - y] \\\\ \\ \\text { Input: } x_{0} \\\\ \\text { Layer1 output: } x_{1} = f_{1} \\left( W_{1} x_{0} \\right) \\\\ \\text { Layer2 output: } x_{2} = f_{2} \\left( W_{2} x_{1} \\right) \\\\ \\text { Output: } x_{3} = f_{3} \\left( W_{3} x_{2} \\right) \\\\ \\end{array} $$\nFor the sake of implementation, I rewrite this with Matrix.\n$$ \\begin{array}{c} \\text { True: } T \\\\ \\text { Objective: } \\mathbb{E}[T - Y] \\\\ \\\\ \\text { Input: } X_{0} \\\\ \\text { Layer1 output: } X_{1} = f_{1} \\left( X_{0} W_{1} \\right) \\\\ \\text { Layer2 output: } X_{2} = f_{2} \\left( X_{1} W_{2} \\right) \\\\ \\text { Output: } X_{3} = f_{3} \\left( X_{2} W_{3} \\right) \\\\ \\end{array} $$\nand the each dimension is as follows\nin : id layer_1 : h1 layer_2 : h2 out : od backpropagation As a loss fuction, I use customized-MSE:\n$$ \\mathbb{E}[T - Y] = \\frac{1}{2} |X_3 - T|_2^2 $$\nThen, let\u0026rsquo;s derive the derivative w.r.t each Weight.\nw.r.t. $W_{3}$\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial W_{3}} \u0026amp;= \\frac{\\partial E}{\\partial X_{3}} \\frac{\\partial X_{3}}{\\partial W_{3}} \\\\ \u0026amp;=\\left(X_{3}-T\\right) \\frac{\\partial X_{3}}{\\partial W_{3}} \\\\ \u0026amp;=\\left[\\left(X_{3}-T\\right) \\circ f_{3}^{\\prime}\\left( X_{2} W_{3} \\right)\\right] \\frac{\\partial X_{2} W_{3} }{\\partial W_{3}} \\\\ \u0026amp;= X_{2}^{T} \\delta_{3} \\\\ \\text { where } \\delta_{3} \u0026amp;=\\left[\\left(X_{3}-T\\right) \\circ f_{3}^{\\prime}\\left( X_{2} W_{3} \\right)\\right] \\\\ \\end{aligned} $$\n$\\frac{\\partial E}{\\partial W_{3}}$ must have the same dimensions as $W_3 ~(h2 \\times od)$. $\\delta_3$ is $n \\times od$. And $X_2$ is $n \\times h2$\nw.r.t. $W_{2}$\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial W_{2}} \u0026amp;= \\frac{\\partial E}{\\partial X_{3}} \\frac{\\partial X_{3}}{\\partial W_{2}} \\\\ \u0026amp;=\\left(X_{3}-T\\right) \\frac{\\partial X_{3}}{\\partial W_{2}} \\\\ \u0026amp;=\\left[\\left(X_{3}-t\\right) \\circ f_{3}^{\\prime}\\left( X_{2} W_{3} \\right)\\right] \\frac{\\partial\\left( X_{2} W_{3} \\right)}{\\partial W_{2}} \\\\ \u0026amp;=\\delta_{3} \\frac{\\partial\\left(X_{2} W_{3} \\right)}{\\partial W_{2}} \\\\ \u0026amp;=\\delta_{3} \\frac{\\partial\\left( X_{2} W_{3} \\right)}{\\partial X_{2}} \\frac{\\partial X_{2}}{\\partial W_{2}} \\\\ \u0026amp;= \\delta_{3} W_{3}^{T} \\frac{\\partial X_{2}}{\\partial W_{2}} \\\\ \u0026amp;=\\left[\\delta_{3} W_{3}^{T} \\circ f_{2}^{\\prime}\\left( X_{1} W_{2} \\right)\\right] \\frac{\\partial X_{1} W_{2} }{\\partial W_{2}} \\\\ \u0026amp;= X_{1}^{T} \\delta_{2} \\\\ \\text { where } \\delta_{2} \u0026amp;=\\left[ \\delta_{3} W_{3}^{T} \\circ f_{2}^{\\prime}\\left( X_{1} W_{2} )\\right)\\right] \\\\ \\end{aligned} $$\n$\\frac{\\partial E}{\\partial W_{2}}$ must have the same dimensions as $W_2 ~(h1 \\times h2)$. $\\delta_2$ is $n \\times h2$. And $X_1$ is $n \\times h1$\nw.r.t. $W_{1}$\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial W_{1}} \u0026amp;=\\left[W_{2}^{T} \\delta_{2} \\circ f_{1}^{\\prime}\\left( X_{0} W_{1} \\right)\\right] X_{0}^{T} \\\\ \u0026amp;= X_{0}^{T} \\delta_{1} \\end{aligned} $$\n$\\frac{\\partial E}{\\partial W_{1}}$ must have the same dimensions as $W_1 ~(id \\times h1)$. $\\delta_1$ is $n \\times h1$. And $X_0$ is $n \\times id$\ncode It\u0026rsquo;s time to implement this. As a dataset, I use iris from sklearn. For the sake of validation of our implementation, I utilized the loss and accuracy.\n References  Vector, Matrix, and Tensor Derivatives by Erik Learned-Miller  ","permalink":"https://kiwamizamurai.github.io/posts/2022-04-25/","summary":"Abstruct In this post, I am going to explain how mathematics is involved with Neural Network and how to implement it with only numpy.\nContent this post was written when I was a junior student. Therefore, there might be some mistakes. Please leave the comment below.\nPrerequisites  rudimentary calculus knowledge  Chain Rule Jacobi-Matrix    This sections help you learn to take derivatives of vectors, matrices, and higher order tensors, thereby making you understand the following part.","title":"How to implement NN from scratch"},{"content":"Abstract I\u0026rsquo;ll show you how to allow markdown to display latex. This website uses hugo and it does not have the ability to render latex by default.\n$e^2$\nIn addition, I write down a way to solve a little latex problem.\nContent Setup First of all, create a file layouts/partials/extend_head.html with the following content.\n{{ if or .Params.math .Site.Params.math }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;!-- The loading of KaTeX is deferred to speed up page rendering --\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- To automatically render math in text elements, include the auto-render extension: --\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; onload=\u0026#34;renderMathInElement(document.body);\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- for inline --\u0026gt;\u0026gt; \u0026lt;script\u0026gt; document.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, function() { renderMathInElement(document.body, { delimiters: [ {left: \u0026#34;$$\u0026#34;, right: \u0026#34;$$\u0026#34;, display: true}, {left: \u0026#34;$\u0026#34;, right: \u0026#34;$\u0026#34;, display: false} ] }); }); \u0026lt;/script\u0026gt; {{ end }} Then, add the parameter in order to enable MathJax on config.yml as follows:\nparams: math: true That\u0026rsquo;s all. Now I can use latex on hugo.\n$$ \\gamma^2+\\theta^2=\\omega^2 $$\nProblem However, there is a tiny problem, which is that the writing style is a little bit different from pure latex on markdown.\n Many markdown processors use the underscore (_) to indicate italics (one at the beginning and one at the end of the text to be italicized). So when your math contains two underscores, Markdown removes them and inserts \u0026hellip;tags (or something equivalent to that) before sending the page to the browser. MathJax doesn\u0026rsquo;t process math that contains HTML tags, so the resulting (modified) math is not typeset. The usual solution is to use a backslash to prevent the underscore from being processed by Markdown, so use _ in place of _ in your math. You may also need to double some backslashes (e.g., \\ may need to be entered as \\\\ in a Markdown document).\n The quote comes from stackoverflow, namely I have to replace _ with \\_. There is one more thing, new line requires \\\\\\ instead of \\\\\nReferences  Using KaTeX in hugo markdown latex driving me crazy with hugo on github How to enable Math Typesetting in PaperMod? · Issue #236 · adityatelange/hugo-PaperMod FAQs · adityatelange/hugo-PaperMod Wiki  ","permalink":"https://kiwamizamurai.github.io/posts/2022-03-06/","summary":"Abstract I\u0026rsquo;ll show you how to allow markdown to display latex. This website uses hugo and it does not have the ability to render latex by default.\n$e^2$\nIn addition, I write down a way to solve a little latex problem.\nContent Setup First of all, create a file layouts/partials/extend_head.html with the following content.\n{{ if or .Params.math .Site.Params.math }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;!-- The loading of KaTeX is deferred to speed up page rendering --\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.","title":"How to enable latex on PaperMod"}]