[{"content":"Abstruct With the development of modern data stacks, it is getting way easy to establish an useful data infrastructure. A lot of people spent much time to investigate and build them. However, as for dbt x aws, there are no resources on the internet. Therefore, I am going to give you a lecture about that.\nGoal I reckon that providing the results makes you understood more efficiently, so pasting some pictures.\nSome people might be surprised because of the multiple databases. Actually, the idea behind the dbt is to use single database.\n https://docs.getdbt.com/docs/faqs/connecting-to-two-dbs-not-allowed  Content First of all, I am going to split my data-warehouse into 4 layers for better data management.\n raw layer  store data as raw format, responsible for copying the datasource   interface layer  canonicalize data   warehouse layer  build the specific domain data by aggregation   mart layer  store data for reverseETL    Prerequisites In addition, I assume that the required infra resources such as athena-database have already been created. In my case, those are produced by terraform. The sample code is located on here.\n https://github.com/kiwamizamurai/tmp  Steps Ok we are ready. The instruction itself is straightforward.\n install dbt-athena and create project  mkdir tmp cd tmp poetry init poetry add dbt-core poetry add dbt-athena-adapter configure dbt as follows\n‚ùØ poetry run dbt init my_dbt_project 23:02:56 Running with dbt=1.2.0 23:02:56 Creating dbt configuration folder at /Users/your_name/.dbt Enter a name for your project (letters, digits, underscore): my_dbt_project Which database would you like to use? [1] athena (Don\u0026#39;t see the one you want? https://docs.getdbt.com/docs/available-adapters) Enter a number: 1 s3_staging_dir (S3 location to store Athena query results and metadata): s3://hogehoge-bucket/athena_output_location/ region_name (AWS region of your Athena instance): ap-northeast-1 schema (Specify the schema (Athena database) to build models into (lowercase only)): some_athena_database_for_default database (Specify the database (Data catalog) to build models into (lowercase only)): awsdatacatalog 23:04:22 Profile my_dbt_project written to /Users/your_name/.dbt/profiles.yml using target\u0026#39;s profile_template.yml and your supplied values. Run \u0026#39;dbt debug\u0026#39; to validate the connection. 23:04:22 Your new dbt project \u0026#34;my_dbt_project\u0026#34; was created! For more information on how to configure the profiles.yml file, please consult the dbt documentation here: https://docs.getdbt.com/docs/configure-your-profile One more thing: Need help? Don\u0026#39;t hesitate to reach out to us via GitHub issues or on Slack: https://community.getdbt.com/ Happy modeling! create tmp/my_dbt_project/profiles.yml for the connection of athena  By default, this configuration file is created at /Users/your_name/.dbt/profiles.yml if you belongs to some team, it might be a good idea to share this. And I do so.    my_dbt_project: target: dev outputs: dev: type: athena s3_staging_dir: s3://hogehoge-bucket/athena_output_location/ region_name: ap-northeast-1 schema: some_athena_database_for_default database: awsdatacatalog poll_interval: 5 work_group: some_athena_work_group # enforce_workgroup_configuration = false num_retries: 1 confirm the connection is healthy  poetry run dbt debug --project-dir ./ --profiles-dir ./ create a macro tmp/my_dbt_project****/****macros****/****get_custom_schema.sql for custom schema so that we can use multiple databases  {% macro generate_schema_name(custom_schema_name, node) -%} {%- set default_schema = target.schema -%} {%- if custome_schema_name is none -%} {{ default_schema }} {%- else -%} {{ custom_schema_name | trim }} {%- endif -%} {%- endmacro %} stratify the directory by modifying the models section of tmp/my_dbt_project/dbt_project.yml  the right-hand side is the name for actual aws_athena_database resource the left-hand side is the folder name corresponding to the resource  tmp/my_dbt_project/models/{raw,interface,warehouse,mart}      models: my_dbt_project: raw: schema: raw +materialized: table interface: schema: interface +materialized: table warehouse: schema: warehouse +materialized: table mart: schema: mart +materialized: table  prepare the sample athena-dataset, athena-table and csv file\n create athena-database which is called raw create athena-table in raw database which is called sample  image that name sample is the name of some particular table which is integrated from any datasource Parameter Location should be s3://hogehoge-bucket/athena_output_location/raw/sample/   upload sample.csv on s3://hogehoge-bucket/athena_output_location/raw/sample/    create tmp/my_dbt_project/models/raw/sample_source.yaml\n  version: 2 sources: - name: raw tables: - name: sample identifier: \u0026#34;\\\u0026#34;sample\\\u0026#34;\u0026#34; # like this columns: - name: col1 - name: col2 write a sample sql for the interface layer tmp/my_dbt_project/models/interface/sample.sql  need to specify external_location    {{ config( materialized=\u0026#39;table\u0026#39;, format=\u0026#39;textfile\u0026#39;, external_location=\u0026#39;s3://hogehoge-bucket/athena_output_location/interface/sample/\u0026#39; ) }} select col1 from {{source(\u0026#39;raw\u0026#39;, \u0026#39;sample\u0026#39;)}} execute command  poetry run dbt run --project-dir ./ --profiles-dir ./  go into the aws console and confirm that athena-database and athena-table is created  miscellaneous When you create your own data-warehouse with your like-minded peers, you must want a coding rule on SQL. In this section, I am going to show you how to prepare team development.\n install library  poetry add sqlfluff  write a setting of sql linter  cd my_dbt_project touch .sqlfluff touch .sqlfluffignore .sqlfluff is like this\n[sqlfluff:rules] tab_space_size = 4 max_line_length = 80 indent_unit = space comma_style = trailing [sqlfluff:rules:L010] capitalisation_policy = lower [sqlfluff:rules:L011] # Aliasing preference for tables aliasing = explicit [sqlfluff:rules:L012] # Aliasing preference for columns aliasing = explicit [sqlfluff:rules:L014] capitalisation_policy = lower unquoted_identifiers_policy = column_aliases [sqlfluff:rules:L016] ignore_comment_lines = True max_line_length = 100 tab_space_size = 2 [sqlfluff] templater = dbt dialect = athena [sqlfluff:templater:dbt] project_dir = . profiles_dir = . profile = my_dbt_project target = dev .sqlfluffignore is like this\ndbt_packages/ macros/  confirm sqlfluff works well  ‚ùØ poetry run sqlfluff lint ./models/mart/should_be_formatted.sql == [my_dbt_project/models/mart/should_be_formatted.sql] FAIL L: 1 | P: 1 | L050 | Files must not begin with newlines or whitespace. L: 10 | P: 1 | L046 | Jinja tags should have a single whitespace on either | side: {{config(materialized=\u0026#39;table\u0026#39;)}} L: 14 | P: 5 | L036 | Select targets should be on a new line unless there is | only one select target. L: 16 | P: 5 | L036 | Select targets should be on a new line unless there is | only one select target. L: 20 | P: 1 | L010 | Keywords must be lower case. L: 20 | P: 1 | L036 | Select targets should be on a new line unless there is | only one select target. L: 20 | P: 11 | L012 | Implicit/explicit aliasing of columns. L: 20 | P: 28 | L012 | Implicit/explicit aliasing of columns. All Finished üìú üéâ! Furthermore, I use this linter and formatter with pre-commit.\n.pre-commit-config.yaml\nrepos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v4.0.1 hooks: - id: trailing-whitespace - id: end-of-file-fixer - id: check-added-large-files args: [\u0026#39;--maxkb=60000\u0026#39;] - id: check-toml - id: check-yaml - id: check-json - id: detect-aws-credentials - id: detect-private-key - repo: https://github.com/sqlfluff/sqlfluff rev: 1.3.0 hooks: - id: sqlfluff-lint args: [\u0026#34;--dialect\u0026#34;, \u0026#34;athena\u0026#34;] additional_dependencies: [\u0026#34;dbt-athena-adapter\u0026#34;, \u0026#34;sqlfluff-templater-dbt\u0026#34;] If you have any questions or requests, please leave the comment below. And, I‚Äôll be happy if you like and share this post. Thank you.\nReferences  https://github.com/Tomme/dbt-athena https://github.com/sqlfluff/sqlfluff https://docs.aws.amazon.com/athena/latest/ug/workgroups-troubleshooting.html https://zenn.dev/tenajima/articles/f4f99caab361c1 https://pre-commit.com/  ","permalink":"https://kiwamizamurai.github.io/posts/2022-08-25/","summary":"Abstruct With the development of modern data stacks, it is getting way easy to establish an useful data infrastructure. A lot of people spent much time to investigate and build them. However, as for dbt x aws, there are no resources on the internet. Therefore, I am going to give you a lecture about that.\nGoal I reckon that providing the results makes you understood more efficiently, so pasting some pictures.","title":"Build Modern Data Warehouse with dbt-athena"},{"content":"Abstruct In this post, I am going to explain how mathematics is involved with Neural Network and how to implement it with only numpy.\nContent this post was written when I was a junior student. Therefore, there might be some mistakes. Please leave the comment below.\nPrerequisites  rudimentary calculus knowledge  Chain Rule Jacobi-Matrix    This sections help you learn to take derivatives of vectors, matrices, and higher order tensors, thereby making you understand the following part.\nderivative As you know, this equation holds.\n$$\\begin{array}{c}\\vec{y}=\\vec{x} W \\\\ \\frac{d \\vec{y}}{d \\vec{x}}=W \\end{array}$$\nWhat is the derivative? What does it mean? In short, derivative is how sensitivie the specific variable with regard to a certain variable is. Jacobian matrix is a good example.\n$$J_f(x) = \\left[\\frac{\\partial f}{\\partial x_{1}} \\cdots \\frac{\\partial f}{\\partial x_{n}}\\right] = \\left[\\begin{array}{ccc}\\frac{\\partial f_{1}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_{m}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_{m}}{\\partial x_{n}} \\end{array}\\right]$$\nThe shape is $m \\times n$ due to the fact that the number of all combination of variables is $m \\times n$.\nvecotr by matrix What happens if I differentiate a vector by a matrix. The possible number of combinations of each variable will be\n$$\\texttt{length of vector} \\times \\texttt{shape of matrix}$$\n(vector varies along one coordinate while matrix varies along two coordinates)\nLet\u0026rsquo;s validate this step by step\n$$\\vec{y}_{3} = \\vec{x}_{1} W_{1,3} + \\vec{x}_{2} W_{2,3} + \\ldots + \\vec{x}_{D} W_{D, 3}$$\n$$\\frac{\\partial \\vec{y}_{3}}{\\partial W_{7,8}}=0 ~~~~ \\frac{\\partial \\vec{y}_{3}}{\\partial W_{2,3}}=\\vec{x}_{2}$$\nIn general, when the index of the $\\vec{y}$ component is equal to the second index of $W$, the derivative will be non-zero, but will be zero otherwise. We can write:\n$$\\frac{\\partial\\vec{y}_{j}}{\\partial W_{i, j}} = \\vec{x}_{i}$$\nbut the other elements of the 3-d array will be 0. If we let $F$ represent the 3d array representing the derivative of $\\vec{y}$ with respect to $W$, where\n$$F_{i, j, k}=\\frac{\\partial \\vec{y}_{i}}{\\partial W_{j,k}}$$\nthen\n$$F_{i, j, i}=\\vec{x}_{j}$$\nThe thing is that all other entries of $F$ are actually zero. Finally, if we define a new two-dimensional array G as\n$$G_{i, j}=F_{i, j, i}$$\nwe can see that all of the information we need about $F$ can be stored in $G$, and that the non-trivial portion of $F$ is really two-dimensional, not three-dimensional.\nmatrix by matrix In implementations of machine learning, we have to deal with a matrix instead of a single vector.\nLet‚Äôs assume that each individual $\\vec{x}$ is a row vector of length $D$, and that $X$ is a two-dimensional array with $N$ rows and $D$ columns. $W$, as in our last example, will be a matrix with $D$ rows and $C$ columns. $Y$ , given by\n$$ Y=X W $$\nwill also be a matrix, with $N$ rows and $C$ columns. Thus, each row of $Y$ will give a row vector associated with the corresponding row of the input $X$.\n$$ \\frac{\\partial Y_{a, b}}{\\partial X_{c, d}} = \\begin{cases} 0 \u0026amp; (a \\neq c) \\\\ not ~ 0 \u0026amp; (otherwise) \\end{cases} $$\nFurthermore, we can see that\n$$\\frac{\\partial Y_{i, j}}{\\partial X_{i, k}}=\\frac{\\partial }{\\partial X_{i, k}} \\left(\\sum_{k=1}^{D} X_{i, k} W_{k, j}\\right)=W_{k, j}$$\ndoesn‚Äôt depend at all upon which row of Y and X we are comparing. This is the same as\n$$ \\frac{\\partial Y_{i,:}}{\\partial X_{i,:}}=W^T $$\nseutp I assume the following architecture.\n$$ \\begin{array}{c} \\text { True: } t \\\\ \\text { Loss: } \\mathbb{E}[t - y] \\\\ \\ \\text { Input: } x_{0} \\\\ \\text { Layer1 output: } x_{1} = f_{1} \\left( W_{1} x_{0} \\right) \\\\ \\text { Layer2 output: } x_{2} = f_{2} \\left( W_{2} x_{1} \\right) \\\\ \\text { Output: } x_{3} = f_{3} \\left( W_{3} x_{2} \\right) \\\\ \\end{array} $$\nFor the sake of implementation, I rewrite this with Matrix.\n$$ \\begin{array}{c} \\text { True: } T \\\\ \\text { Objective: } \\mathbb{E}[T - Y] \\\\ \\\\ \\text { Input: } X_{0} \\\\ \\text { Layer1 output: } X_{1} = f_{1} \\left( X_{0} W_{1} \\right) \\\\ \\text { Layer2 output: } X_{2} = f_{2} \\left( X_{1} W_{2} \\right) \\\\ \\text { Output: } X_{3} = f_{3} \\left( X_{2} W_{3} \\right) \\\\ \\end{array} $$\nand the each dimension is as follows\nin : id layer_1 : h1 layer_2 : h2 out : od backpropagation As a loss fuction, I use customized-MSE:\n$$ \\mathbb{E}[T - Y] = \\frac{1}{2} |X_3 - T|_2^2 $$\nThen, let\u0026rsquo;s derive the derivative w.r.t each Weight.\nw.r.t. $W_{3}$\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial W_{3}} \u0026amp;= \\frac{\\partial E}{\\partial X_{3}} \\frac{\\partial X_{3}}{\\partial W_{3}} \\\\ \u0026amp;=\\left(X_{3}-T\\right) \\frac{\\partial X_{3}}{\\partial W_{3}} \\\\ \u0026amp;=\\left[\\left(X_{3}-T\\right) \\circ f_{3}^{\\prime}\\left( X_{2} W_{3} \\right)\\right] \\frac{\\partial X_{2} W_{3} }{\\partial W_{3}} \\\\ \u0026amp;= X_{2}^{T} \\delta_{3} \\\\ \\text { where } \\delta_{3} \u0026amp;=\\left[\\left(X_{3}-T\\right) \\circ f_{3}^{\\prime}\\left( X_{2} W_{3} \\right)\\right] \\\\ \\end{aligned} $$\n$\\frac{\\partial E}{\\partial W_{3}}$ must have the same dimensions as $W_3 ~(h2 \\times od)$. $\\delta_3$ is $n \\times od$. And $X_2$ is $n \\times h2$\nw.r.t. $W_{2}$\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial W_{2}} \u0026amp;= \\frac{\\partial E}{\\partial X_{3}} \\frac{\\partial X_{3}}{\\partial W_{2}} \\\\ \u0026amp;=\\left(X_{3}-T\\right) \\frac{\\partial X_{3}}{\\partial W_{2}} \\\\ \u0026amp;=\\left[\\left(X_{3}-t\\right) \\circ f_{3}^{\\prime}\\left( X_{2} W_{3} \\right)\\right] \\frac{\\partial\\left( X_{2} W_{3} \\right)}{\\partial W_{2}} \\\\ \u0026amp;=\\delta_{3} \\frac{\\partial\\left(X_{2} W_{3} \\right)}{\\partial W_{2}} \\\\ \u0026amp;=\\delta_{3} \\frac{\\partial\\left( X_{2} W_{3} \\right)}{\\partial X_{2}} \\frac{\\partial X_{2}}{\\partial W_{2}} \\\\ \u0026amp;= \\delta_{3} W_{3}^{T} \\frac{\\partial X_{2}}{\\partial W_{2}} \\\\ \u0026amp;=\\left[\\delta_{3} W_{3}^{T} \\circ f_{2}^{\\prime}\\left( X_{1} W_{2} \\right)\\right] \\frac{\\partial X_{1} W_{2} }{\\partial W_{2}} \\\\ \u0026amp;= X_{1}^{T} \\delta_{2} \\\\ \\text { where } \\delta_{2} \u0026amp;=\\left[ \\delta_{3} W_{3}^{T} \\circ f_{2}^{\\prime}\\left( X_{1} W_{2} )\\right)\\right] \\\\ \\end{aligned} $$\n$\\frac{\\partial E}{\\partial W_{2}}$ must have the same dimensions as $W_2 ~(h1 \\times h2)$. $\\delta_2$ is $n \\times h2$. And $X_1$ is $n \\times h1$\nw.r.t. $W_{1}$\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial W_{1}} \u0026amp;=\\left[W_{2}^{T} \\delta_{2} \\circ f_{1}^{\\prime}\\left( X_{0} W_{1} \\right)\\right] X_{0}^{T} \\\\ \u0026amp;= X_{0}^{T} \\delta_{1} \\end{aligned} $$\n$\\frac{\\partial E}{\\partial W_{1}}$ must have the same dimensions as $W_1 ~(id \\times h1)$. $\\delta_1$ is $n \\times h1$. And $X_0$ is $n \\times id$\ncode It\u0026rsquo;s time to implement this. As a dataset, I use iris from sklearn. For the sake of validation of our implementation, I utilized the loss and accuracy.\n References  Vector, Matrix, and Tensor Derivatives by Erik Learned-Miller  ","permalink":"https://kiwamizamurai.github.io/posts/2022-04-25/","summary":"Abstruct In this post, I am going to explain how mathematics is involved with Neural Network and how to implement it with only numpy.\nContent this post was written when I was a junior student. Therefore, there might be some mistakes. Please leave the comment below.\nPrerequisites  rudimentary calculus knowledge  Chain Rule Jacobi-Matrix    This sections help you learn to take derivatives of vectors, matrices, and higher order tensors, thereby making you understand the following part.","title":"How to implement NN from scratch"},{"content":"Abstract I\u0026rsquo;ll show you how to allow markdown to display latex. This website uses hugo and it does not have the ability to render latex by default.\n$e^2$\nIn addition, I write down a way to solve a little latex problem.\nContent Setup First of all, create a file layouts/partials/extend_head.html with the following content.\n{{ if or .Params.math .Site.Params.math }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;!-- The loading of KaTeX is deferred to speed up page rendering --\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- To automatically render math in text elements, include the auto-render extension: --\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; onload=\u0026#34;renderMathInElement(document.body);\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- for inline --\u0026gt;\u0026gt; \u0026lt;script\u0026gt; document.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, function() { renderMathInElement(document.body, { delimiters: [ {left: \u0026#34;$$\u0026#34;, right: \u0026#34;$$\u0026#34;, display: true}, {left: \u0026#34;$\u0026#34;, right: \u0026#34;$\u0026#34;, display: false} ] }); }); \u0026lt;/script\u0026gt; {{ end }} Then, add the parameter in order to enable MathJax on config.yml as follows:\nparams: math: true That\u0026rsquo;s all. Now I can use latex on hugo.\n$$ \\gamma^2+\\theta^2=\\omega^2 $$\nProblem However, there is a tiny problem, which is that the writing style is a little bit different from pure latex on markdown.\n Many markdown processors use the underscore (_) to indicate italics (one at the beginning and one at the end of the text to be italicized). So when your math contains two underscores, Markdown removes them and inserts \u0026hellip;tags (or something equivalent to that) before sending the page to the browser. MathJax doesn\u0026rsquo;t process math that contains HTML tags, so the resulting (modified) math is not typeset. The usual solution is to use a backslash to prevent the underscore from being processed by Markdown, so use _ in place of _ in your math. You may also need to double some backslashes (e.g., \\ may need to be entered as \\\\ in a Markdown document).\n The quote comes from stackoverflow, namely I have to replace _ with \\_. There is one more thing, new line requires \\\\\\ instead of \\\\\nReferences  Using KaTeX in hugo markdown latex driving me crazy with hugo on github How to enable Math Typesetting in PaperMod? ¬∑ Issue #236 ¬∑ adityatelange/hugo-PaperMod FAQs ¬∑ adityatelange/hugo-PaperMod Wiki  ","permalink":"https://kiwamizamurai.github.io/posts/2022-03-06/","summary":"Abstract I\u0026rsquo;ll show you how to allow markdown to display latex. This website uses hugo and it does not have the ability to render latex by default.\n$e^2$\nIn addition, I write down a way to solve a little latex problem.\nContent Setup First of all, create a file layouts/partials/extend_head.html with the following content.\n{{ if or .Params.math .Site.Params.math }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;!-- The loading of KaTeX is deferred to speed up page rendering --\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.","title":"How to enable latex on PaperMod"}]